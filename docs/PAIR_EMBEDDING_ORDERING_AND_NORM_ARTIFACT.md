### Summary

We observed strong clustering structure in PCA plots of **pair embeddings** built as ordered concatenations \([emb_a, emb_b]\):

- **HA + NA only**: PCA shows **2 distinct clusters**.
- **PB2 + HA + NA**: PCA shows **~6 clusters**.

This matches the number of **ordered function pairs** (e.g., HA→NA vs NA→HA; with 3 functions there are 3×2 = 6 ordered directions). This strongly suggests that PCA (and potentially the model) is capturing **slot identity / ordering artifacts** rather than isolate-related signal.

This document summarizes the assumptions, discoveries, and recommended next steps.

---

### Context / Definitions

- **Pair embedding representation**: Concatenation \([emb_a, emb_b]\) where each \(emb\) is an ESM-2 (mean-pooled) embedding vector.
- **Ordered pairs**: (A,B) and (B,A) yield different model inputs because concatenation is order-sensitive.
- **Task semantics**: “same isolate?” is an **undirected** relation; order should not matter.

---

### Key discoveries (grounded in this codebase)

#### Ordered concatenation is used for the PCA/UMAP split-overlap plots

`src/analysis/visualize_dataset_stats.py` generates pair embeddings with:
- `create_pair_embeddings_concatenation(pairs, ...)` → returns `[emb_a, emb_b]`.

#### Positive vs negative generation uses different ordering behavior

In `src/datasets/dataset_segment_pairs.py`:

- **Positives**: generated from within-isolate pairs using `itertools.combinations(...)` over the isolate’s rows. This yields a **systematic direction** `(row_a, row_b)` determined by the (often stable) row order.
- **Negatives**: generated by randomly sampling `row_a` from isolate1 and `row_b` from isolate2. This yields **random direction**: both (A,B) and (B,A) occur.

If direction correlates (even weakly) with label, an order-sensitive model (MLP on concatenation) can learn a **shortcut**:
- “Direction looks typical of positives ⇒ predict positive”
- “Direction looks atypical ⇒ predict negative”

This can inflate performance and reduce generalization, especially on “harder” filtered datasets.

---

### Evidence collected so far

#### PCA cluster counts align with ordered function-pair counts

The “2 clusters” for HA/NA and “6 clusters” for PB2/HA/NA is exactly what we expect if the dominant PCA axis is “which function is in slot A vs slot B”.

#### Diagnostics implemented in plotting

We added explicit ordering diagnostics and metadata summaries to the plotting pipeline:

- **File**: `src/analysis/visualize_dataset_stats.py`
- **Outputs per dataset run** (under `plots/`):
  - `pair_embedding_order_diagnostics.json`
  - `pair_embeddings_pca_by_seg_pair.png`
  - `pair_embeddings_pca_by_func_pair.png`

The JSON includes:
- `flip_rate_pc1_sign`: how often PC1 sign changes when swapping halves (same PCA model).
- magnitude-based metrics: `mean_abs_delta_pc1`, `mean_abs_delta_pc2`, `mean_l2_delta_pca2d`
- `corr_pc1_ab_vs_ba`: correlation between PC1 before/after swapping
- correlations between PC1 and `||emb_a||`, `||emb_b||`, `||emb_a||/||emb_b||`

These are designed to quantify whether “swap (A,B)→(B,A)” maps points across PCA clusters (order-driven structure) and whether embedding norms are acting as a proxy for slot/function identity.

---

### Assumptions (explicit)

- **A1**: The isolate-matching task is **undirected**; there is no biological meaning to “A before B”.
- **A2**: PCA clusters that align with `func_a→func_b` / `seg_a→seg_b` are likely dominated by **representation artifacts** (slot identity) rather than isolate signal.
- **A3**: If dataset generation creates systematic direction for positives but random direction for negatives, the model can learn a **direction shortcut**.
- **A4**: Embedding norms may encode function/segment identity or other nuisance variation; if norms correlate strongly with PCA axes, the model may also exploit magnitude-based shortcuts.

---

### Risks / Why this matters

- **Hidden training shortcut**: A small MLP can exploit ordering/norm artifacts without learning isolate-specific compatibility.
- **Misleading stress-test results**: performance may appear strong on broad datasets but collapse on filtered/homogeneous datasets.
- **Fragility at inference**: if ordering at inference differs from training (e.g., swapped input halves), predictions can change dramatically.

---

### Recommended next steps (in order)

#### Step 0 — Prove or disprove shortcut effects with direct audits (fast)

1) **Directed label rates** (on the actual `*_pairs.csv` used for training):
- Compute \(P(label=1 \mid func_a\to func_b)\) and compare to \(P(label=1 \mid func_b\to func_a)\).
- Repeat for `seg_a→seg_b`.
- If materially different, direction is a label proxy.

Implementation:

Run:

```bash
python -m src.analysis.audit_pair_direction_and_norm_leakage \
  --dataset_dir /path/to/.../runs/dataset_<bundle>_<timestamp> \
  --embeddings_file /path/to/.../master_esm2_embeddings.h5
```

Outputs (written under the dataset run dir):

- `plots/leakage_audits/summary.json`
- `plots/leakage_audits/*_direction_asymmetry_*_top50.csv`
- `plots/leakage_audits/*_norm_only_baseline_metrics.json`

2) **Swap sensitivity test (strongest sanity check)**:
- Take a trained model and evaluate test performance twice:
  - with `[emb_a, emb_b]`
  - with swapped `[emb_b, emb_a]`
- Large performance drop indicates reliance on direction.

3) **Norm-only baseline** (upper bound on “cheating”):
- Train a trivial classifier using only:
  - `||emb_a||`, `||emb_b||`, `||emb_a||/||emb_b||`
  - optionally plus one-hot of `func_a/func_b` to see a worst-case proxy ceiling

#### Step 1 — Canonicalize direction during dataset generation (cleanest fix)

Make every unordered pair appear in exactly one deterministic orientation for both positives and negatives.

Suggested canonicalization key:
- Prefer **`seq_hash`** (consistent with `pair_key` semantics), or
- Use `brc_fea_id` if `seq_hash` is unavailable (less ideal if IDs don’t map 1:1 to identical sequences across sources).

Critical requirement:
- Swap *all* `*_a` / `*_b` fields consistently:
  - `brc_*`, `seq_*`, `seg_*`, `func_*`, `seq_hash_*`, `assembly_id_*`

This removes “direction leakage” while keeping model input format unchanged.

Implementation status:

- `src/datasets/dataset_segment_pairs.py` now canonicalizes stored orientation for every pair
  (positives and negatives) via `canonicalize_pair_orientation()` using:
  - primary: `seq_hash_a` vs `seq_hash_b`
  - tie-break: `brc_a` vs `brc_b`

#### Step 2 — Add model-side order invariance as a safety net (optional)

Even with canonicalization, consider making the model robust to accidental swaps:

- Symmetric features (classic):
  - `a+b`, `|a-b|` (and optionally elementwise `a*b`)
- Or training/inference ensembling:
  - predict on both `(a,b)` and `(b,a)` and average

This is a larger behavioral change and should be done with clean A/B comparisons.

#### Step 3 — Address norm shortcuts carefully (optional)

If norm-based artifacts are strong:

- Consider **L2-normalizing each half** before concatenation:
  - `a/||a||`, `b/||b||`
- Optional compromise for analysis:
  - keep original norms as explicit scalar features appended to the vector

Do not do this blindly; treat it as an experiment because norm can sometimes carry useful signal.

---

### Open questions

- How different are `func_a→func_b` label rates vs reversed direction in real training CSVs?
- Does swapping halves at inference materially change model predictions/performance?
- After canonicalization, how much performance remains (i.e., what is the true isolate signal)?

