## Stress-test visuals (quick sanity checks)

This doc describes the quick visualization stress tests we discussed with the team to check that performance is not driven by obvious confounders (split artifacts, host/year/subtype/geography/passage).

### What was added/updated

- **Split-overlap plot (pair embeddings)**: a 2D projection of the *actual model input vectors* (concatenated embedding pairs `[emb_a, emb_b]`), **colored + marked by split** (train/val/test). Goal: train/val/test should **overlap** (not be separable).
- **Confounder plots (single embeddings)**: 2D projection of *single-protein embeddings* (restricted to the dominant function in the sampled data, e.g. HA), colored by:
  - host
  - hn_subtype
  - year bin (<=2019 vs 2020+)
  - geo_location_clean
  - passage
- **Dataset composition plots**: distributions by split for `geo_location_clean` and `passage` in addition to host/year/hn_subtype.

All plots are generated by `src/analysis/visualize_dataset_stats.py` and (by default) are called automatically from `src/datasets/dataset_segment_pairs.py` after dataset creation.

### How to run

#### Option A: regenerate plots for an existing dataset run

```bash
cd /nfs/lambda_stor_01/data/apartin/projects/cepi/viral-segmatch
source "$(conda info --base)/etc/profile.d/conda.sh"
conda activate cepi

# Example: replace with your actual run directory
python src/analysis/visualize_dataset_stats.py \
  --config_bundle flu_2024 \
  --bundle flu_2024 \
  --dataset_dir /nfs/lambda_stor_01/data/apartin/projects/cepi/viral-segmatch/data/datasets/flu/July_2025/runs/dataset_flu_2024_YYYYMMDD_HHMMSS
```

Plots are saved to:
- dataset run dir: `.../plots/`
- results dir: `results/{virus}/{data_version}/{bundle}/dataset_analysis/plots/`

#### Option B: generate a new dataset (plots run automatically)

```bash
python src/datasets/dataset_segment_pairs.py --config_bundle flu_2024
```

### UMAP vs PCA fallback

If `umap-learn` is installed, the split-overlap and confounder plots use **UMAP**.
If not, the code falls back to **PCA** and writes `*_pca.png`.

To enable UMAP:

```bash
pip install umap-learn
```

### How to interpret (what “good” vs “bad” looks like)

#### 1) Pair-embedding split overlap: `pair_embeddings_split_overlap_umap.png` (or `_pca.png`)

- **Good**: train/val/test points are well-mixed with no clear boundaries.
- **Bad**: obvious separation by split (e.g., a “test island”). This suggests a split artifact or a strong confounder correlated with split membership.

#### 2) Confounder plots (single embeddings; dominant function)

Files look like: `sequence_embeddings_{umap|pca}_{function}_by_{confounder}.png`

- **If you see strong clustering by host/subtype/year**: that means the *embedding space itself* encodes these factors strongly (not surprising biologically). It becomes plausible that a pair-classifier can exploit correlations between “same isolate” and “same subtype/year/host”.
- **If geography/passage clusters strongly**: could be data-collection artifacts (lab pipelines, surveillance programs) leaking into sequences indirectly, or could be true phylogeographic structure.

The point is not “no structure exists” (it will), but whether the structure aligns with the performance deltas you’re seeing when filtering by host/year/subtype.

### Recommended quick follow-ups (team discussion)

- If the split-overlap plot looks “bad”, do we want to:
  - change split strategy (e.g., split by subtype/year buckets), or
  - keep split but interpret results as conditional on that confounder?
- For year, do we prefer **pre/post 2020** bins (COVID surveillance effect) or a rolling temporal split (train past → test future)?
- For geography, do we want country-level only, or “US state else country” (current `geo_location_clean` policy)?

