You provide very elaborate responses, which is often very helpful. However, for this set of questions, provide only the minimum required info for me to move on. Yet, it should contain all the required info.

viral-segmatch/
├── conf/              # Config files for experiments, training
├── data/              # Raw and processed data (use DVC if large)
├── models/            # Trained models & logs (tracked via W&B/DVC)
├── notebooks/         # Jupyter notebooks for exploration
├── results/           # Evaluation results and logs
├── scripts/           # Automation scripts
├── src/               # Core code (ML models, data processing)
│   ├── data/          # Data processing modules
│   ├── models/        # Model implementations
│   └── utils/         # Utility functions
├── tests/             # Unit and integration tests
├── .github/workflows/ # CI/CD pipelines (e.g., GitHub Actions)
├── Dockerfile         # Container definition
├── docker-compose.yml # Service orchestration
├── README.md          # Project overview, setup, usage
├── requirements.txt   # Dependencies
└── .gitignore         # Ignore unnecessary files (datasets, checkpoints)

On May 15, 2025, I spotted an issue with GCF_013086535.1.qual.gto. Jim provided an updated file. The older file is stored in
/nfs/lambda_stor_01/data/apartin/projects/cepi/viral-segmatch/data/raw/Anno_Updates/April_2025/bunya-from-datasets/_misc

# ================
Step-by-Step Plan
# ================

# -----------------------------------
1. Selecting a Protein Language Model
# -----------------------------------
We can leverage pretrained protein sequence models that learn embeddings of protein sequences.

ESM-2 (Evolutionary Scale Modeling) is highly effective at capturing protein properties and relationships.

A transformer-based model trained on large protein datasets Outputs embeddings that capture relationships between amino acids.

# --------------------------------
2. Modifying ESM-2 for Our Purpose
# --------------------------------
How ESM-2 Works:

Input: A protein sequence (e.g., "MKTFL...")
Output: A numerical embedding of that sequence, capturing its biochemical properties

We modify this approach as follows:

Instead of feeding in single protein sequences, we feed in pairs (or triplets) of protein sequences.
The model should predict whether the given proteins are linked (i.e., from the same viral individual).
We add a classification head on top of ESM-2, where the final output is a probability score (0-1) indicating the likelihood that the segments belong to the same virus.

# --------------------
3. Dataset Preparation
# --------------------
Dataset Structure:

For Lassa, each sample should have:
Segment 1 (Protein Sequence from L segment)
Segment 2 (Protein Sequence from S segment)
Label (1 if they belong to the same virus, 0 otherwise)

For Bunyavirales, it would be segment triplets (small, medium, large).

Steps to Build the Dataset:
a) Extract Protein Sequences
- Translate DNA sequences into protein sequences using standard genetic code. --> do we have protein seqeuences for the segments??
- Extract N, GP, L, Z proteins from the respective segments. This means that from each segment (e.g., a protein structure or a sequence region), we are specifically selecting the proteins labeled as N, GP, L, and Z. These could be specific proteins relevant to your problem (e.g., viral proteins or domain-specific proteins) that are essential for classification. The exact extraction method depends on how the segments are defined. --> does it mean that we need to label each segment with one of these (available) protein? If yes, how this info will be used? Also, what do you mean by "selecting the proteins" (are there any other proteins)? 

b) Pair (or Triple) the Sequences
- Use metadata and accession numbers to find segment pairs that are highly likely to be linked.
- Label true pairs/triplets as 1, and create negative samples (randomly mismatched segments) labeled as 0.

c) Tokenization for ESM-2
- ESM-2 requires sequences to be tokenized (e.g., amino acids A, C, D → numerical representations).
- Use ESM’s tokenizer to encode the sequences.

d) Train a Classifier on Top of ESM-2 Embeddings
- Pass the paired embeddings through a classification layer.
- Fine-tune ESM-2 so that the embeddings are optimized for linking genome segments.

# -------------------
4. Training the Model
# -------------------
- Loss Function: Binary Cross-Entropy (for Lassa) or Categorical Cross-Entropy (for Bunyavirales with 3 segments).
- Evaluation Metric: Accuracy, ROC-AUC, and F1-score to measure classification performance.
- Data Augmentation: Introduce minor mutations in protein sequences to make the model robust. --> check if there is
    any literature on this?? check what does and does not make sense

# -----------------------
5. Expansion Beyond Lassa
# -----------------------
Once we establish a working model on Lassa:
- Expand to Bunyavirales (2-3 segments)
- Later, tackle Influenza (8 segments)
- Potentially, generalize to other segmented viruses

Summary of the Approach
- Use ESM-2, a powerful protein language model
- Input: Pairs/triplets of protein sequences from viral genome segments
- Output: Probability score indicating whether the segments belong to the same viral individual
- Dataset: Labeled protein segment pairs (Lassa) or triplets (Bunyavirales)
- Fine-tune ESM-2 and train a classifier on top of it




# =========
Details
# =========

# -------------------------------------------
1. Tokenization & Model Input/Output in ESM-2
# -------------------------------------------
Tokenization:
- When passing a protein sequence through the tokenizer, it converts amino acids (A, C, D, etc.) into numerical tokens.
- The length of the tokenized representation depends on the sequence length, not fixed size. DL models, especially those with batch processing, generally require fixed-length inputs. However, protein sequences vary in length. There are a few ways to handle this:
    - Using Transformers' Attention Masking: If using ESM-2, it supports variable-length input by using attention masks instead of requiring fixed-length sequences.
    - Padding & Truncation: Sequences are padded to the maximum length in a batch or truncated if too long.
    - Packing Sequences: Some architectures (e.g., LSTMs) can handle variable-length sequences by using masks.
- ESM-2 uses a Transformer architecture, so it processes sequences like a language model (akin to BERT or GPT for text).

ESM-2 Output:
- Per-residue embeddings: A high-dimensional vector for each amino acid in the sequence (useful for per-site predictions). In this context, a "residue" refers to an amino acid in the protein sequence. Since proteins are composed of amino acid chains, each position in the sequence corresponds to an amino acid residue, and ESM-2 provides a high-dimensional vector (embedding) for each residue.
- Global embedding: A fixed-size representation of the full protein sequence (usually taken from the last-layer output).
- This global embedding is what we’d use for segment linking.

So in our case:
- Convert protein sequences into embeddings using ESM-2. --> so does that mean that for 2-segment models, we'll have 2 ESM models, each for each segment??
- Combine embeddings from two segments (Lassa) or three segments (Bunyavirales).
- Feed the combined representation into a classifier that predicts whether they belong to the same virus.

# -------------------------------------
2. How to Modify for 3-Segment Viruses?
# -------------------------------------
The key challenge in moving from a 2-segment model (Lassa) to a 3-segment model (Bunyavirales) is in how we combine and process embeddings.

Lassa (2-segment model)
- Input: Two protein sequences (from L and S segments).
- Processing:
    - Convert each sequence into an embedding using ESM-2.
    - Concatenate/merge embeddings.
    - Pass through a classifier that outputs a probability score (0 or 1) indicating whether they belong to the same viral individual.

Bunyavirales (3-segment model)
- Input: Three protein sequences (from S, M, and L segments).
- Challenges:
    - More complex relationships between segments.
    - Different viruses have different segment organizations.

Architectural Adjustments:
- Instead of a pairwise classifier, we now need a multi-segment classifier.
- Possible approaches:
    1. Concatenation Approach: Simply concatenate all three embeddings and pass them through a classifier.
    2. Pairwise Attention Approach:
        Train a model to compare (S-M), (S-L), and (M-L) pairwise.
        Aggregate pairwise scores into a final decision.
    3. Transformer Fusion Approach:
        Treat segment embeddings as a "sentence" and apply self-attention across them.


# ---------------------------------------------------------
3. Can We Reuse the 2-Segment Model for 3-Segment Training?
# ---------------------------------------------------------
Yes! We can transfer knowledge from the 2-segment Lassa model to the 3-segment Bunyavirales model in a few ways:

1. Use Pretrained ESM-2 on Proteins from Lassa
- The 2-segment model will learn useful protein embeddings.
- These embeddings can be used directly in the 3-segment model.

2. Fine-tune the 2-segment Classifier on 3-segment Data
- Train the Lassa model to classify pairs (S-L).
- Later, extend the classifier to handle triplets (S-M-L) by fine-tuning.

3. Multi-Step Training Strategy:
- Step 1: Train Lassa 2-segment model.
- Step 2: Train Bunyavirales in a pairwise manner first (S-M, M-L, S-L).
- Step 3: Combine pairwise predictions into a 3-segment classifier.





I solved the issue with the models. Let's get back to discuss the design for Option 3: "3. Deep Learning on Protein Sequences" for our sm1. Can you summarize the objective of this project, including detials for the approach, so that I can present it later to an LLM. Please use simple formatting so that I'll be able to copy-paste it to google doc. Note: I'll use ESM-2 from huggingface transformers.


