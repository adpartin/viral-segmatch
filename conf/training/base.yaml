# Base training configuration (reusable)
batch_size: 16
learning_rate: 0.001 # 1e-3
epochs: 120
# optimizer: 'adam'  # Options: 'adam', 'adamw', 'sgd'
optimizer: 'adamw'  # Options: 'adam', 'adamw', 'sgd'
weight_decay: 0.01  # L2 regularization (0.01 recommended for AdamW, 0.0001 for Adam)
momentum: 0.9  # For SGD only
use_lr_scheduler: true  # Enable learning rate scheduling
lr_scheduler: 'reduce_on_plateau'  # Options: 'reduce_on_plateau', 'cosine', 'step'
lr_scheduler_patience: 10  # For reduce_on_plateau: epochs to wait before reducing LR
lr_scheduler_factor: 0.5  # Factor to reduce LR by (for reduce_on_plateau)
lr_scheduler_min_lr: 1e-6  # Minimum learning rate

# Model architecture
hidden_dims: [512, 256, 64]
dropout: 0.2

# Pre-MLP options (applied to each embedding before interaction)
pre_mlp_mode: none  # none | shared | slot_specific | shared_adapter | slot_norm
pre_mlp_dims: null  # e.g., [1280, 512, 256] this is the dimension of the pre-MLP output
adapter_dims: null  # e.g., [128] this is the dimension of the adapter output

# Interaction spec: which features to compute from (emb_a, emb_b)
interaction: concat  # concat | diff | prod | unit_diff | concat+unit_diff | etc.

# Training control
patience: 15  # Early stopping patience
early_stopping_metric: 'f1'  # Metric for early stopping: 'loss' (lower is better), 'f1' and 'auc' (higher is better)
threshold_metric: null  # Threshold optimization metric; null (no optimization; use 0.5), 'f1' (default), 'f0.5' (emphasize precision), 'f2' (emphasize recall); This metric is used to determine the threshold for the binary classifier; the threshold is the metric value that is used to classify the pairs as positive or negative

# Diagnostics
eval_swapped_test: false  # If true, also evaluate the trained model on swapped test inputs (B,A) and write test_predicted_swapped.csv
